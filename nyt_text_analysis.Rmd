---
title: "Analyzing Text from New York Times Article Abstracts"
author: "Clara Fong"
date: "`r lubridate::today()`"
output: 
 github_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

```

```{r, include=FALSE}
# Load Packages
library(tidyverse)
library(stringr)
library(httr)
library(jsonlite)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidymodels)
library(tidytext)

theme_set(theme_minimal())

set.seed(1234)
```

## Summary of Report

Extending on last week's homework assignment, I wanted to continue looking at New York Times articles related to international migrants, broadly speaking. For homework 8, I had spent a majority of my efforts figuring out how to create a GET request and work with the New York Times API, so most of my analysis was based on variables of the articles themselves that I could pull out (e.g., year of published article and the word count, the frequency of regions discussed in the articles, if publication date can predict word count,  etc.). This time, I wanted to focus on the content of the actual article and examine how journalists are talking about migration in these articles across the world.


## Accessing API and Creating Data Frame

### Data Source:

I used the [New York Times Developers API](https://developer.nytimes.com/apis) to build this data frame. After creating an account and reading the documentation on how to submit a query, I was able to filter for all stories in the past ten years (2010-2020) relating to migrants. Note: this slightly differs from homework 8 because I only looked at the past five years for that asssignment. I wanted to specifically look at the Foreign "newsdesk", which is another way to filter for stories because I was curious to see what was being addressed internationally. Details of how I created the data frame and code can be found on my [R markdown file](nyt.Rmd). To convert the `.json` file to a data frame, I leaned heavily on Professor Terman's PLSC 31101 [course website](https://plsc-31101.github.io/course/collecting-data-from-the-web.html#writing-api-queries).

```{r get request function, include = FALSE}

# Create an Rprofile page to store API keys using:
#file.edit(here::here(".Rprofile"))

# Enter your NYT API key (available here when you sign up for an account: https://developer.nytimes.com/)
  #it should look like: options(nyt_key = "YOURKEYHERE")

# Set relevant parameters for GET request
key <- getOption("nyt_key") 
base.url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json"
search_term <- "migrant"
filter <- 'news_desk:("Foreign")'
begin <- '20100101'
end <- '20201231'

# Testing GET Request on Single Request
articles <- GET(base.url, query = list(`q` = search_term,
                                       `fq` = filter,
                                       `begin_date` = begin,
                                       `end_date` = end,
                                       `api-key` = key))

# Parse info from JSON format
response <- content(articles, "text")  
response_df <- fromJSON(response, simplifyDataFrame = TRUE, flatten = TRUE) # will need this for later


# Create function for specified parameters above
nyt_api <- function(page){
  base.url = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
    # Send GET request
    articles <- GET(base.url, query = list(`q` = search_term,
                                       `fq` = filter,
                                       `begin_date` = begin,
                                       `end_date` = end,
                                       `api-key` = key,
                                       `page` = page)) #using same GET request, only difference is adding page
    
    # Parse response to JSON
    response <- content(articles, "text")  
    response_df <- fromJSON(response, simplifyDataFrame = TRUE, flatten = TRUE) #same as above
    
    message(glue::glue("Scraping page: ", as.character(page))) # print message to track progress
    
    return(response_df$response$docs) # returns article info stored as df
}

# Test the function on next set of pages
docs <- nyt_api(page = 2) # yay it works!
```

```{r building df, include=FALSE, cache = TRUE}

# Extract total hits
hits = response_df$response$meta$hits

# Store number of pages (there are 10 hits per page)
pages = ceiling(hits/10)

# Modify function to slow down scraping
nyt_api_slow <- slowly(f = nyt_api, rate = rate_delay(1))

# Create new df with all articles that match hit using iterative function
articles_df <- map_dfr(.x = (1:pages), ~nyt_api_slow(page = .))
```

```{r data cleaning}
# Main df
cleaned_articles_df <- articles_df %>% 
  select(snippet,
         lead_paragraph,
         word_count,
         pub_date,
         headline.main,
         subsection_name) %>% 
  filter(subsection_name != "What in the World",
         subsection_name != "Politics",
         subsection_name == recode(subsection_name, Canada = "Americas")) %>% 
  mutate(pub_date = ymd_hms(pub_date),
         year = year(pub_date),
         subsection_name = as.factor(subsection_name))

# in this cleaned df, there are several areas to extract text from (e.g., the title, the lead paragraph etc.)
# im going to look at the leading paragraphs of each article
```

### Tidying the Text
(notes)

```{r tidied text, message=FALSE, warning=FALSE}
# Select only for the lead paragraph, it's already in tibble() format
tidied_text <- cleaned_articles_df %>% 
  select(pub_date, 
         lead_paragraph,
         subsection_name) %>% 
  tibble(line = 1:464, text = lead_paragraph) %>% 
  unnest_tokens(word, lead_paragraph) %>% 
  select(!text)

# Load stopwords data
data(stop_words)

tidied_text <- tidied_text %>% 
  anti_join(stop_words)

```

## Text Analysis

### Most Common Words in Leading Paragraphs

(text)

```{r common words viz}
# Count and Plot the Frequency of Certain Words
tidied_text %>% 
  count(word, sort = TRUE) %>% 
  filter(word != "migrants") %>% #removing because it's redundant given all articles are about migrants
  arrange(desc(n)) %>% 
  head(10) %>% 
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col()
```

### Most Common Words used for Each Continent 

```{r common words cont}

# Calculate tf-idf scores for words mention in each continent area
continent_tf_idf <- tidied_text %>%
  count(subsection_name, word) %>%
  bind_tf_idf(term = word, document = subsection_name, n = n)

# visualize the top N terms per character by tf-idf score
continent_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(subsection_name) %>%
  slice_max(n = 10, order_by = tf_idf, with_ties = FALSE) %>%
  # resolve ambiguities when same word appears for different characters
  ungroup() %>%
  mutate(word = reorder_within(x = word, by = tf_idf, within = subsection_name)) %>%
  ggplot(mapping = aes(x = word, y = tf_idf)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  labs(
    title = "Most important words in Articles about Migration",
    subtitle = "By Continent",
    x = NULL,
    y = "tf-idf",
    caption = "Source: NYT API"
  ) +
  facet_wrap(~subsection_name, scales = "free") +
  coord_flip()

```


### Article Sentiment Analysis

(text) -- need to cite both dictionary lexicons
* changed the df to use snippet instead of lead paragraph to get rid of the country/cities that keep popping up, see if the frequency is different

```{r sentiment, message=FALSE}

# Creating sentiment df
sentiment_text <- cleaned_articles_df %>% 
  select(pub_date, 
         snippet, # differs from tidied_text here by using "snippet" instead of "lead_paragraph"
         subsection_name) %>% 
  tibble(line = 1:464, text = snippet) %>% 
  unnest_tokens(word, snippet) %>% 
  select(!text) %>% 
  anti_join(stop_words)

# Get Sentiment
afinn_sentiment <- get_sentiments("afinn")
nrc_sentiment <- get_sentiments("nrc")

articles_sentiment <- tidied_text %>% 
  inner_join(nrc_sentiment) %>% 
  count(subsection_name, index = pub_date, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% #could try to use pivot_longer here
  mutate(sentiment = positive - negative)

  ggplot(articles_sentiment, aes(x = index, y = sentiment, fill = subsection_name)) +
  geom_col() +
    facet_wrap(~subsection_name)
  
  
#  ??? wtf is happening why is it not graphing
  
  

```




## Session info

```{r, echo = TRUE}
devtools::session_info()
```

```

