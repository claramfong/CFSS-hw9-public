---
title: "Analyzing Text from New York Times Articles"
author: "Clara Fong"
date: "`r lubridate::today()`"
output: 
 github_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)

```

```{r, include=FALSE}
# Load Packages
library(tidyverse)
library(stringr)
library(httr)
library(jsonlite)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidymodels)
library(tidytext)
library(wordcloud)
library(ggwordcloud)
library(reshape2)

theme_set(theme_minimal())

set.seed(1234)
```

## Summary of Report

Extending on last week's homework assignment, I wanted to continue looking at New York Times articles related to international migrants, broadly speaking. For homework 8, I had spent a majority of my efforts figuring out how to create a GET request and work with the New York Times API, so most of my analysis was based on variables of the articles themselves that I could pull out (e.g., year of published article and the word count, the frequency of regions discussed in the articles, if publication date can predict word count,  etc.). This time, I wanted to focus on the content of the actual article and examine how journalists are talking about migration in these articles across the world.


## Accessing API and Creating Data Frame

### Data Source

I used the [New York Times Developers API](https://developer.nytimes.com/apis) to build this data frame. After creating an account and reading the documentation on how to submit a query, I was able to filter for all stories in the past ten years (2010-2020) relating to migrants. Note: this slightly differs from homework 8 because I only looked at the past five years for that asssignment. I wanted to specifically look at the Foreign "newsdesk", which is another way to filter for stories because I was curious to see what was being addressed internationally. Details of how I created the data frame and code can be found on my [R markdown file](nyt.Rmd). To convert the `.json` file to a data frame, I leaned heavily on Professor Terman's PLSC 31101 [course website](https://plsc-31101.github.io/course/collecting-data-from-the-web.html#writing-api-queries).

```{r get request function, include = FALSE}

# Create an Rprofile page to store API keys using:
#file.edit(here::here(".Rprofile"))

# Enter your NYT API key (available here when you sign up for an account: https://developer.nytimes.com/)
  #it should look like: options(nyt_key = "YOURKEYHERE")

# Set relevant parameters for GET request
key <- getOption("nyt_key") 
base.url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json"
search_term <- "migrant"
filter <- 'news_desk:("Foreign")'
begin <- '20100101'
end <- '20201231'

# Testing GET Request on Single Request
articles <- GET(base.url, query = list(`q` = search_term,
                                       `fq` = filter,
                                       `begin_date` = begin,
                                       `end_date` = end,
                                       `api-key` = key))

# Parse info from JSON format
response <- content(articles, "text")  
response_df <- fromJSON(response, simplifyDataFrame = TRUE, flatten = TRUE) # will need this for later


# Create function for specified parameters above
nyt_api <- function(page){
  base.url = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
    # Send GET request
    articles <- GET(base.url, query = list(`q` = search_term,
                                       `fq` = filter,
                                       `begin_date` = begin,
                                       `end_date` = end,
                                       `api-key` = key,
                                       `page` = page)) #using same GET request, only difference is adding page
    
    # Parse response to JSON
    response <- content(articles, "text")  
    response_df <- fromJSON(response, simplifyDataFrame = TRUE, flatten = TRUE) #same as above
    
    message(glue::glue("Scraping page: ", as.character(page))) # print message to track progress
    
    return(response_df$response$docs) # returns article info stored as df
}

# Test the function on next set of pages
nyt_api(page = 2) # yay it works!
```

```{r building df, include=FALSE, cache = TRUE}

# Extract total hits
hits = response_df$response$meta$hits

# Store number of pages (there are 10 hits per page)
pages = ceiling(hits/10)

# Modify function to slow down scraping
nyt_api_slow <- slowly(f = nyt_api, rate = rate_delay(1))

# Create new df with all articles that match hit using iterative function
articles_df <- map_dfr(.x = (1:pages), ~nyt_api_slow(page = .))
```

```{r data cleaning}
# Cleaning original df, before making it a useful tidy text formal
cleaned_articles_df <- articles_df %>% 
  select(snippet,
         lead_paragraph,
         word_count,
         pub_date,
         headline.main,
         subsection_name) %>% 
  filter(subsection_name != "What in the World",
         subsection_name != "Politics",
         subsection_name == recode(subsection_name, Canada = "Americas")) %>% 
  mutate(pub_date = ymd_hms(pub_date),
         pub_date = date(pub_date),
         year = year(pub_date),
         subsection_name = as.factor(subsection_name))

# in this cleaned df, there are several areas to extract text from (e.g., the title, the lead paragraph etc.)
# im going to look at the leading paragraphs of each article
```

### Tidying the Text

Because each row in the new data frame consists of the details of one articles, I needed to select only necessary information from the cells with text in them. This meant I could select from the title, the lead paragraph, or the "snippet," which I think is used for social media/advertising purposes. I chose to analyze the lead paragraphs for the overall sentiment among all the articles in the past 10 years. To tidy the data, I started by making each word in the leading paragraph a row, then filtered the `stop_words` (ones that hold little semantic meaning) from the text.

```{r tidied text, message=FALSE, warning=FALSE}
# Select only for the lead paragraph, it's already in tibble() format
tidied_text <- cleaned_articles_df %>% 
  select(pub_date, 
         lead_paragraph,
         subsection_name) %>% 
  tibble(text = lead_paragraph) %>%  
  unnest_tokens(word, lead_paragraph) %>% 
  select(!text)

# Load stopwords data
data(stop_words)

tidied_text <- tidied_text %>% 
  anti_join(stop_words)

```

## Text Analysis

### Most Common Words in Leading Paragraphs

Starting off, it would be interesting to look at what are the most common words used in these articles in the past 10 years before looking at any kind of sentiment analysis. I'm curious to see if we will capture any meaningful verbs or adjectives used to describe migrants/migrant story.

```{r common words viz}
# Count and Plot the Frequency of Certain Words
tidied_text %>% 
  count(word, sort = TRUE) %>% 
  filter(word != "migrants") %>% #removing bc it's redundant given all articles are filter to mention migrants
  arrange(desc(n)) %>% 
  head(10) %>% 
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col(fill = "steelblue") +
  labs(
    title = "Most Frequent Words Used in Articles about Migrants",
    subtitle = "In 2010-2020",
    x = "Frequency",
    y = "Words",
    caption = "Source: NYT API"
  )
```

Expectedly, the most commonly used word is Europe/European, which is interesting considering the migrant European crisis didn't happen until 2015-2016. This suggests that there was really a spike in articles talking about migrants during this period, much more so than in the previous 5 years (2010-2015) and perhaps even after. 

It should also be noted that there are a few semantically less meaningful words included in this chart (e.g. friday, people, thousand). Given that this analysis will only take the top used word, we will need to do more of an analysis of sentiment and not just the text itself to further evaluate what kind of language is being used to describe migrants generally and around the world.


### Most Common Words used for Each Continent by `tf-idf` Score

As we conducted above, one way to quantify the language of these text documents is to look a term's frequency (how often it is mentioned). However, as we also saw above, this doesn't always allow us to take the most important words even after filtering for stop words. So, we can use a inverse document frequency to decrease the weight of commonly used words and increase the weight of words not used as much. 

```{r common words cont}

# Calculate tf-idf scores for words mention in each continent area in lead paragraph
continent_tf_idf <- tidied_text %>%
  count(subsection_name, word) %>%
  bind_tf_idf(term = word, document = subsection_name, n = n)

# Visualize the top n terms per character by tf-idf score
continent_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% # return words as unique factors in word col
  group_by(subsection_name) %>%
  slice_max(n = 10, order_by = tf_idf, with_ties = FALSE) %>%
  # resolve ambiguities when same word appears for different characters
  ungroup() %>%
  mutate(word = reorder_within(x = word, by = tf_idf, within = subsection_name)) %>%
  # plotting the graph
  ggplot(mapping = aes(x = word, 
                       y = tf_idf,
                       fill = subsection_name)) +
  geom_col(show.legend = FALSE) +
  scale_fill_brewer(palette = "Accent") +
  scale_x_reordered() +
  labs(title = "Most important words in Articles about Migration",
       subtitle = "By Continent",
       x = NULL,
       y = "tf-idf",
       caption = "Source: NYT API") +
  facet_wrap(~subsection_name, scales = "free") +
  coord_flip()
```

After creating a `tf-idf` score, the plots identify word that are important to a text but not used *too* commonly. I have further gone in and grouped these by continents. As we can see, the "important words" by `tf-idf` are actually countries or regions within the continents. That being said, we still see some interesting phrases such as how the Middle East includes "coast" or "boat", and Australia includes "contentious" and "detainees".

I suppose this analysis tells us more about *where* events relating to migration are happening most frequently than *what* is being discussed. Furthermore, segmenting by continent gives us a more nuanced observation than the general word frequency, since we know that there are a disproportionate amount of articles about Europe than other regions.


### Article Sentiment Analysis

For this part, I relied on the [Text mining with R textbook](https://www.tidytextmining.com/tidytext.html) cited in our class materials. 

One thing that was interesting about sentiment analysis is the use of dictionaries, created by previous researchers. Below, I used the [Bing et al.](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) dictionary that dichotomizes words into positive or negative to analyze the general trends of positive/negative sentiment in articles over time. Also note, I also made two different plots to compare the article snippets from their lead paragraphs to see if there might be any obvious difference between the two. My initial guess would be that snippets would have more extreme language (more positive/ negative) because it is used to capture readers' attention.


```{r sentiment data, message=FALSE}

# Get Sentiments from all three dictionaries
afinn_sentiment <- get_sentiments("afinn")
nrc_sentiment <- get_sentiments("nrc")
bing_sentiment <- get_sentiments("bing")

# Creating sentiment df
sentiment_text_lead <- cleaned_articles_df %>% 
  select(pub_date, 
         lead_paragraph, 
         subsection_name) %>% 
  tibble(text = lead_paragraph) %>% 
  unnest_tokens(word, lead_paragraph) %>% 
  select(!text) %>% 
  anti_join(stop_words)

# And again for snippets
# Creating sentiment df
sentiment_text_snippet <- cleaned_articles_df %>% 
  select(pub_date, 
         snippet, # differs from tidied_text here by using "snippet" instead of "lead_paragraph"
         subsection_name) %>% 
  tibble(text = snippet) %>% 
  unnest_tokens(word, snippet) %>% 
  select(!text) %>% 
  anti_join(stop_words)

```


```{r sentiment plots, message = FALSE, fig.show='hold', out.width="50%"}

# Using bing dictionary: What does the net positive and negative look for lead paragraph?
sentiment_text_lead %>% 
  inner_join(bing_sentiment) %>% 
  count(subsection_name, index = pub_date, sentiment) %>% 
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(x = index, 
             y = sentiment, 
             color = subsection_name)) +
  geom_line(size = 0.4, 
            show.legend = FALSE) +
  scale_x_date(date_labels = "%Y %b %d") +
  theme(axis.text.x = element_text(angle = 23)) +
  scale_y_continuous(name="Sentiment Score", limits=c(-5, 5)) +
  facet_wrap(~subsection_name, ncol = 2) +
  labs(
    title = "Migrant Article Lead Paragraph Sentiment by Continent",
    subtitle = "From 2010-2020",
    x = "Year"
  )

# Using bing dictionary: What does the net positive and negative look like for snippet?
sentiment_text_snippet %>% 
  inner_join(bing_sentiment) %>% 
  count(subsection_name, index = pub_date, sentiment) %>% 
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(x = index, y = sentiment, color = subsection_name)) +
  geom_line(size = 0.4, show.legend = FALSE) +
  scale_x_date(date_labels = "%Y %b %d") +
  theme(axis.text.x = element_text(angle = 23)) +
  scale_y_continuous(name="Sentiment Score", limits=c(-5, 5)) +
    facet_wrap(~subsection_name, ncol = 2) +
  labs(
    title = "Migrant Article 'Snippet' Sentiment by Continent",
    subtitle = "From 2010-2020",
    x = "Year"
  )

```

Interestingly, there is more variation and extreme values (both positive and negative) in the lead paragraph's sentiment than in the "snippet" introduction lines. We can also see that, on average, the words used to describe migrants in stories and/or their conditions are overwhelmingly negative. There are rarely any articles across all the key continents that score high on this positive sentiment dictionary. For the most part, when the New York Times is talking about migrants, they are not talking about them in a positive way. This is likely due to the fact that migrant working and living conditions are pretty precarious and also easily exploitable. Migrants also encompass documented and undocumented migrants (e.g., migrant workers, refugees, asylum seekers, etc.) as we note with the European migrant crisis.

Furthermore, we can go ahead and look at the most commonly used negative and positive words in these articles.

```{r sentiment words, message = FALSE, fig.show='hold', out.width="50%"}
# most commonly used negative and positive words
sentiment_text_lead %>% 
  inner_join(bing_sentiment) %>% 
  group_by(sentiment) %>%
  count(word) %>% 
  group_by(sentiment) %>%
  slice_max(order_by = n, n = 10) %>% 
  filter(word != "crisis",
         word != "led") %>% # there's too many uses of these and it's skewing the data
  mutate(word = reorder_within(word, n, sentiment)) %>% 
  ggplot(aes(x = word, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values=c("steelblue", "#E69F00")) +
  scale_x_reordered() + # renaming the x axis ticks
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = NULL,
       y = "Word Importance to Sentiment Score") +
  coord_flip()

sentiment_text_snippet %>% 
  inner_join(bing_sentiment) %>% 
  group_by(sentiment) %>%
  count(word) %>% 
  group_by(sentiment) %>%
  slice_max(order_by = n, n = 10) %>% 
  filter(word != "crisis",
         word != "led") %>% 
  mutate(word = reorder_within(word, n, sentiment)) %>% 
  ggplot(aes(x = word, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values=c("steelblue", "#E69F00")) +
  scale_x_reordered() +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = NULL,
       y = "Word Importance to Sentiment Score") +
  coord_flip()
```

As we can see, the most frequently used terms for both positive and negative have some faults in this dictionary. Of course, without context, some of these words seem like they would be a positive or negative. However, we know that, for example, it is unlikely the term "trump" as an adjective like the dictionary might assume and rather it was likely referring to former President Trump. Another limitation of this analysis is that the positive and negative "sentiment" doesn't give us much insight into the agency vs. object of these stories. For example, "attacks" is one of the most commonly used terms in these articles, but it is unclear whether this is referring to attacks made *onto* migrants or attacks *by* migrants. In this basic analysis, we lose out some of the nuance in these stories by only looking at the terms. 


### Wordclouds

Finally, one thing we did not explicitly cover in class was how to make these word clouds, which are another representation of our text data (it was, however, covered in the exercises but we did not have time to go over it in class). Below, I have created a word cloud that polarizes positive and negative words based on the Bing et al. dictionary (again).

```{r word cloud, message = FALSE, fig.show='hold', out.width="50%"}

# using base R word cloud package
tidied_text %>%
  inner_join(bing_sentiment) %>%
  filter(word != "migrant",
         word != "migrants",
         word != "crisis") %>% 
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick", "steelblue4"),
                   max.words = 100) 

# using ggwordcloud package
tidied_text %>%
  inner_join(bing_sentiment) %>% 
  filter(word != "migrant",
         word != "migrants",
         word != "crisis") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  slice_max(order_by = n, n = 100) %>% # only the top 100 words
  ggplot(aes(label = word, size = n,
           x = sentiment, color = sentiment)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 20) +
  scale_x_discrete(breaks = NULL) +
  ggtitle ("Most frequent terms in NYT Articles on Migrants")

```

Visually, these two plots more or less do the same thing, the left-hand side plot was generated in base R's package `wordcloud`, and the right-hand side plot was generated in ggplot's version, `ggwordcloud`. Both word clouds are another visualization method of the most frequently used words, positive and negative as determined by Bing et al., in the New York Times articles on migration in the last 10 years.

Overall, the plots here show that there are far more negative words than positive ones when reporting on migrants. As mentioned previously, one possible reason this might be the case is that there are more negative things happen to migrants than there are positive ones. This kind of sentiment analysis says less about how reporters talk about migrants and perhaps more about what actually is happening to migrants around the world.


## Session info

```{r, echo = TRUE}
devtools::session_info()
```

```

